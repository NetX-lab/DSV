================================================================================
Sparse Attention Kernel Evaluation
================================================================================
Configuration:
   â€¢ Batch Size: 1
   â€¢ Sequence Length: 64,000
   â€¢ Attention Heads: 4
   â€¢ Head Dimension: 128
   â€¢ Low-rank Dimension: 16
   â€¢ Group Size: 32
   â€¢ Sparsity Per Head: [0.92, 0.92, 0.92, 0.92]
================================================================================

Warming up GPU kernels...
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%
 âœ… Passed the correctness check, failed_ratio: 0.0%

Running evaluation benchmarks...

======================================== RUN 1/3 ========================================
Step 1: Preparing attention matrices...
Step 2: Low-rank approximation for sparse KV selection...
Step 3: Running sparse attention with performance measurement...
Step 4: Running full attention baseline for comparison...
Step 5: Validating correctness against masked full attention baseline...
 âœ… Passed the correctness check, failed_ratio: 0.0%

ðŸ“Š EVALUATION RESULTS:
  Performance Metrics:
    Low-rank KV Selection:
      - Time:              5.20 ms
    Forward Pass:
      - Sparse Attention:  4.04 ms
      - Full Attention:    18.83 ms
      - Speedup:           4.67x
    Backward Pass:
      - Sparse Attention:  77.40 ms
      - Full Attention:    244.17 ms
      - Speedup:           3.15x
    Total Time:
      - Sparse (excl. low-rank): 81.44 ms
      - Sparse (incl. low-rank): 86.64 ms
      - Full Attention:          263.00 ms
      - Speedup (excl. low-rank): 3.23x
      - Speedup (incl. low-rank): 3.04x
  Correctness Metrics:
    âœ… PASSED - Numerical error: 0.0% (within tolerance)
  Configuration:
    - Sparsity (uniform): 92.0% (keeping 8.0% of KV pairs)
    - Selected KV per head: [5120, 5120, 5120, 5120]

======================================== RUN 2/3 ========================================
Step 1: Preparing attention matrices...
Step 2: Low-rank approximation for sparse KV selection...
Step 3: Running sparse attention with performance measurement...
Step 4: Running full attention baseline for comparison...
Step 5: Validating correctness against masked full attention baseline...
 âœ… Passed the correctness check, failed_ratio: 0.0%

ðŸ“Š EVALUATION RESULTS:
  Performance Metrics:
    Low-rank KV Selection:
      - Time:              5.19 ms
    Forward Pass:
      - Sparse Attention:  4.05 ms
      - Full Attention:    18.70 ms
      - Speedup:           4.61x
    Backward Pass:
      - Sparse Attention:  77.54 ms
      - Full Attention:    244.46 ms
      - Speedup:           3.15x
    Total Time:
      - Sparse (excl. low-rank): 81.59 ms
      - Sparse (incl. low-rank): 86.78 ms
      - Full Attention:          263.16 ms
      - Speedup (excl. low-rank): 3.23x
      - Speedup (incl. low-rank): 3.03x
  Correctness Metrics:
    âœ… PASSED - Numerical error: 0.0% (within tolerance)
  Configuration:
    - Sparsity (uniform): 92.0% (keeping 8.0% of KV pairs)
    - Selected KV per head: [5120, 5120, 5120, 5120]

======================================== RUN 3/3 ========================================
Step 1: Preparing attention matrices...
Step 2: Low-rank approximation for sparse KV selection...
Step 3: Running sparse attention with performance measurement...
Step 4: Running full attention baseline for comparison...
Step 5: Validating correctness against masked full attention baseline...
 âœ… Passed the correctness check, failed_ratio: 0.0%

ðŸ“Š EVALUATION RESULTS:
  Performance Metrics:
    Low-rank KV Selection:
      - Time:              5.22 ms
    Forward Pass:
      - Sparse Attention:  4.05 ms
      - Full Attention:    18.65 ms
      - Speedup:           4.60x
    Backward Pass:
      - Sparse Attention:  77.15 ms
      - Full Attention:    248.39 ms
      - Speedup:           3.22x
    Total Time:
      - Sparse (excl. low-rank): 81.20 ms
      - Sparse (incl. low-rank): 86.42 ms
      - Full Attention:          267.04 ms
      - Speedup (excl. low-rank): 3.29x
      - Speedup (incl. low-rank): 3.09x
  Correctness Metrics:
    âœ… PASSED - Numerical error: 0.0% (within tolerance)
  Configuration:
    - Sparsity (uniform): 92.0% (keeping 8.0% of KV pairs)
    - Selected KV per head: [5120, 5120, 5120, 5120]

================================================================================
COMPLETE
================================================================================

Generating performance visualization...

Performance visualization saved as 'sparse_attention_performance_breakdown_20250712_1831.pdf'
